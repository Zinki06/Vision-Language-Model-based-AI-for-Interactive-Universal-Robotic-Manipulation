# LLM2PF6: LLM 기반 객체 인식 및 제스처 기반 타겟 객체 추론 시스템

이 프로젝트는 컴퓨터 비전과 대규모 언어 모델(LLM)을 결합하여, **사용자의 모호한 지시어와 손 제스처를 기반으로 목표 물체(타겟)를 식별**하고, 해당 **목표 위치**를 3D/2D로 추론하는 시스템을 제공합니다.

---

## 목차

1. [전체 프로그램 실행 파이프라인](#전체-프로그램-실행-파이프라인)
2. [주요 모듈 구조](#주요-모듈-구조)
3. [실행 흐름 및 데이터 처리 과정](#실행-흐름-및-데이터-처리-과정)
4. [세부 실행 파이프라인](#세부-실행-파이프라인)
5. [중요 모듈 간 상호작용](#중요-모듈-간-상호작용)
6. [시스템 기능](#시스템-기능)
7. [연구 목적 및 방법](#연구-목적-및-방법)
8. [기술 스택](#기술-스택)
9. [설치 및 실행 방법](#설치-및-실행-방법)

---

## 전체 프로그램 실행 파이프라인

다음은 LLM2PF6 시스템의 주요 실행 파이프라인을 요약한 ASCII 다이어그램입니다:

```
┌────────────────┐     ┌────────────────┐     ┌────────────────┐
│1. 입력 캡처    │ --> │2. 객체 감지    │ --> │3. 깊이 추정    │
│ (웹캠/프레임)   │     │ (YOLOv8)       │     │ (MiDaS)        │
└────────────────┘     └────────────────┘     └────────────────┘
                   ┌────────────────┐     ┌───────────────────┐
                   │4. 제스처 인식  │ --> │5. 지시어 분석     │
                   │ (MediaPipe)    │     │ (LLM, GPT-4o)     │
                   └────────────────┘     └───────────────────┘

                                     ┌────────────────────────┐
                                     │6. 목표 위치 추론       │
                                     │ (3D 좌표, 2D 바운딩)   │
                                     └────────────────────────┘

                                     ┌────────────────────────┐
                                     │7. 결과 시각화          │
                                     │ (OpenCV 등)            │
                                     └────────────────────────┘

                                     ┌────────────────────────┐
                                     │8. 좌표 데이터 저장      │
                                     │ (JSON 파일 등)         │
                                     └────────────────────────┘
```

---

## 주요 모듈 구조

### 1. 메인 애플리케이션 (`main.py`)

- **역할**: 전체 시스템 실행 및 사용자 인터페이스 관리  
- **핵심 클래스**: `InteractiveGoalInferenceApp`  
  - 카메라 입력, 키보드 이벤트 처리  
  - 분석 요청 및 결과 표시  

### 2. 목표 추론 엔진 (`modules/goal_inference.py`)

- **역할**: 시스템의 핵심 엔진  
- **주요 기능**: 
  - 입력 이미지에서 객체 감지  
  - 사용자 의도 해석  
  - 목표 위치 추론(3D/2D)  

### 3. 객체 감지 모듈 (`modules/yolo_detector.py`)

- **역할**: YOLOv8 기반 객체 감지  
- **주요 기능**: 
  - 이미지 내 객체 식별 및 분류  
  - 바운딩 박스 정보 추출  

### 4. 깊이 추정 모듈 (`modules/depth_3d_mapper.py`)

- **역할**: MiDaS 모델을 사용한 단안 깊이 추정  
- **주요 기능**: 
  - 2D 이미지에서 깊이 맵(Depth Map) 생성  
  - 객체의 3D 좌표 추정  

### 5. 제스처 인식 모듈 (`modules/gesture_recognizer.py`)

- **역할**: MediaPipe Hand Landmarker 기반 손 제스처 감지  
- **주요 기능**: 
  - 손 랜드마크 추출 및 포인팅 동작 방향 벡터 계산  

### 6. 프롬프트 처리 파이프라인 (`modules/enhanced_prompting_pipeline.py`)

- **역할**: 사용자 자연어 지시를 구조적으로 해석  
- **주요 기능**:
  - 지시대명사 해석 ("이거", "저거")  
  - 명령 구체화 (추가 정보 보강)  
  - 타겟/레퍼런스 객체 및 방향 추론  

### 7. LLM 인터페이스 (`modules/gpt4o_interface.py`, `modules/llm_interface.py`)

- **역할**: GPT-4o 등 대규모 언어 모델과의 통신  
- **주요 기능**:  
  - 고급 자연어 처리  
  - 맥락 기반 추론 및 응답  

### 8. 공간 분석기 (`modules/spatial_analyzer.py`)

- **역할**: 객체 간 공간적 관계 분석  
- **주요 기능**:  
  - 상대적 위치 계산  
  - 충돌 검사 등  

### 9. 결과 저장 모듈 (`modules/result_storage.py`)

- **역할**: 분석 결과와 좌표 데이터 등을 저장  
- **주요 기능**:  
  - JSON 형식으로 결과 로그 저장  
  - 스냅샷 및 분석 이미지 저장  

### 10. 설정 관리 시스템 (`modules/utils/config_manager.py`, `config/`)

- **역할**: 시스템 설정 로드/관리  
- **주요 기능**:  
  - 여러 소스(파일, 환경 변수, 명령행)에서 설정 로드  
  - 공통 설정 인터페이스 제공  

---

## 실행 흐름 및 데이터 처리 과정

### 1. 애플리케이션 초기화

1. 설정 로드 (환경 변수, YAML/JSON 등)  
2. 카메라 초기화 및 목표추론 엔진 생성  
3. UI 및 이벤트 핸들러 등록  

### 2. 실시간 영상 처리 루프

1. 웹캠에서 프레임 캡처  
2. 제스처 모드 활성 시 손 제스처 실시간 추적  
3. 사용자 키 입력(단축키)에 따라 즉시 동작 수행  

### 3. 스냅샷 캡처 및 분석

1. `'d'` 키 입력 시 스냅샷 촬영  
2. 사용자 프롬프트(텍스트) 입력 또는 음성 입력(STT)  
3. `'a'` 키 입력 시 분석 수행  

### 4. 이미지 분석 파이프라인

1. 객체 감지 (YOLOv8)  
2. 깊이 추정 (MiDaS)  
3. 제스처 인식 (MediaPipe)  
4. LLM 프롬프트 처리 (EnhancedPromptingPipeline)  
5. 최종 목표 위치 계산 (3D 좌표, 2D 바운딩 박스)  

### 5. 결과 표시 및 저장

1. 바운딩 박스, 레이블, 방향 등 시각화  
2. 분석 정보(신뢰도, 방향 등) 표시  
3. JSON 파일로 좌표 데이터 저장  
4. 이미지 결과 저장(옵션)  

---

## 세부 실행 파이프라인

### main.py 진입점 파이프라인

```
┌─────────────────────────────┐
│ 1. 환경 및 설정 초기화       │
│    main() → load_dotenv()   │
│    → parse_arguments()       │
│    → get_config_manager()    │
└─────────────────────────────┘
                |
                v
┌─────────────────────────────┐
│ 2. 애플리케이션 객체 생성 및 │
│    실행 시작                │
│    main() →                 │
│    InteractiveGoalInferenceApp(...)│
│    → app.start()            │
└─────────────────────────────┘
                |
                v
┌─────────────────────────────┐
│ 3. 카메라 초기화 및 루프 진입 │
│    start() →                │
│    _initialize_camera()      │
│    → camera_manager.get_active_camera()│
└─────────────────────────────┘
                |
                v
┌─────────────────────────────┐
│ 4. 이벤트 처리 루프          │
│   (실시간 프레임 처리,       │
│    제스처/키 입력 등)        │
└─────────────────────────────┘
                |
                ├─ 'd': _take_snapshot() + 오디오녹음 토글
                ├─ 'a': _start_analysis() → analyze_snapshot()
                ├─ 'p': 사용자 프롬프트 입력
                ├─ 'g': 제스처 모드 토글
                ├─ 's': 결과 저장
                └─ 'q': 종료
                |
                v
┌─────────────────────────────┐
│ 5. 이미지 분석 파이프라인    │
│    analyze_snapshot() →      │
│    engine.process_image() →  │
│    _process_analysis_results()│
└─────────────────────────────┘
                |
                v
┌─────────────────────────────┐
│ 6. 종료 및 리소스 정리       │
│    _cleanup_resources() →    │
│    audio_recorder.stop_recording()     │
│    gesture_recognizer.close()          │
│    camera_manager.release_all()        │
│    cv2.destroyAllWindows()             │
└─────────────────────────────┘
```

### 이미지 처리 및 목표 추론 파이프라인

`GoalPointInferenceEngine.process_image()`:

```
┌──────────────────────────────────┐
│1. 객체 감지                     │
│   yolo_detector.detect() → dets │
└──────────────────────────────────┘
                |
                v
┌──────────────────────────────────┐
│2. 깊이 추정                     │
│   depth_mapper.estimate_depth() │
│   → depth_map, colored_depth    │
│   → for obj in dets:           │
│       depth_mapper.get_object_3d_position()│
└──────────────────────────────────┘
                |
                v
┌──────────────────────────────────┐
│3. 제스처 인식                   │
│   gesture_recognizer.process_frame() →│
│   gesture_results                │
└──────────────────────────────────┘
                |
                v
┌──────────────────────────────────┐
│4. 타겟/레퍼런스/방향 추론       │
│   4.1 제스처 기반 (활성 시)     │
│       find_pointed_object()     │
│   4.2 LLM 기반 (프롬프트 입력)  │
│       prompting_pipeline.process()│
└──────────────────────────────────┘
                |
                v
┌──────────────────────────────────┐
│5. 목표 위치 계산 (3D/2D)        │
│   calculate_goal_3d_position()  │
│   generate_goal_bounding_box()  │
└──────────────────────────────────┘
                |
                v
┌──────────────────────────────────┐
│6. 결과 시각화 및 저장           │
│   visualization.draw_results()  │
│   result_storage.save_analysis_results()│
└──────────────────────────────────┘
```

---

## 중요 모듈 간 상호작용

### 자연어 처리 파이프라인 (EnhancedPromptingPipeline)

```
사용자 프롬프트
  └───> 1단계: 지시대명사 해석 (_resolve_demonstrative_pronouns)
        └───> 2단계: 명령 구체화 (_enhance_prompt)
               └───> 3단계: 타겟/레퍼런스/방향 추론 (_infer_target_reference_goal)
                      └───> 결과 반환
```

1. **지시대명사 해석**  
   - 예: "이거 저기로 옮겨줘" → "에어팟 저기로 옮겨줘"  
2. **명령 구체화**  
   - 예: "에어팟 저기로 옮겨줘" → "에어팟을 책상 위로 옮겨줘"  
3. **타겟/레퍼런스/방향 추론**  
   - 최종 결과: `{"target_object_id": 3, "reference_object_id": 1, "direction": "above"}`

### 제스처-언어 통합 파이프라인

```
1. 제스처 인식 결과 (gesture_results)
2. 자연어 이해 결과 (nlp_results)
3. 시간적 매칭 (GestureAnalyzer._match_recent_gestures_with_deictic())
4. 제스처 기반 객체 식별 (find_pointed_object())
5. 통합 결과 생성
   if gesture:
       target = pointed_object
   else if nlp_results:
       target = detections[nlp_results.target_object_id]
```

### 3D 좌표 계산 및 목표 위치 추론

1. **타겟/레퍼런스 객체 식별**  
2. **방향 설정** (사용자 입력 또는 기본값)  
3. **3D 좌표 계산**  
4. **2D 바운딩 박스 생성**  
5. **목표 위치 정보 구성**  

---

## 시스템 기능

1. **객체 인식 및 분류**  
   - YOLOv8 활용, 다양한 물체 감지  
2. **3D 공간 매핑**  
   - MiDaS로부터 깊이 맵을 얻고 3D 좌표 추론  
3. **제스처 인식**  
   - 손 랜드마크 추출 및 포인팅 벡터 계산  
4. **자연어 이해**  
   - GPT-4o 기반, 지시대명사와 공간적 명령 처리  
5. **실시간 상호작용**  
   - 웹캠 영상 스트리밍, 키보드 단축키, 음성 입력 지원  

---

## 연구 목적 및 방법

**연구 목표**: “사용자의 모호한 지시어와 손 제스처를 인식하여 정확한 타겟 물체를 식별하고, 해당 물체를 옮길 목표 위치를 3D로 추론”

### 연구 문제

1. **지시대명사**: “이거”, “저거” 등의 모호한 표현  
2. **멀티모달 통합**: 음성(텍스트)와 제스처를 어떻게 연결할 것인가?  
3. **3D 공간 이해**: 단안 카메라만으로 3D 관계를 어떻게 추론할 것인가?  
4. **신뢰도 평가**: 어떻게 추론 결과의 정확도를 사용자에게 안내할 것인가?  

### 해결 방법

1. **3단계 LLM 프롬프팅** (지시대명사 해석 → 명령 구체화 → 최종 추론)  
2. **제스처-언어 매핑** (시간적 근접성 + 포인팅 벡터)  
3. **단안 깊이 추정** (MiDaS + 카메라 파라미터)  
4. **신뢰도 평가** (추론 confidence 점수 제공, 사용자 피드백 루프)  

---

## 기술 스택

- **객체 감지**: YOLOv8, OpenCV  
- **깊이 추정**: MiDaS  
- **제스처 인식**: MediaPipe Hand Landmarker  
- **자연어 처리**: GPT-4o  
- **데이터 처리**: NumPy, PyTorch  
- **음성 처리**: PyAudio, SpeechRecognition  
- **시각화**: OpenCV, Matplotlib  
- **설정 관리**: YAML, JSON  

---

## 설치 및 실행 방법

### 설치

```bash
# 1. 저장소 복제
git clone https://github.com/your-username/LLM2PF6.git
cd LLM2PF6

# 2. 가상환경 생성 및 활성화
python -m venv .venv
source .venv/bin/activate  # MacOS/Linux
# (Windows: .venv\Scripts\activate)

# 3. 의존성 설치
pip install -r requirements.txt

# 4. API 키 설정 (.env 파일 생성)
echo "OPENAI_API_KEY=your_api_key_here" > .env
```

### 실행

```bash
# 기본 설정으로 실행
python main.py

# 옵션을 지정해 실행
python main.py --yolo yolov8l.pt --llm gpt4o --conf 0.15 --output_dir output
```

### 키보드 단축키

- `d`: 스냅샷 촬영 + 음성녹음 시작/종료  
- `a`: 스냅샷 분석 (프롬프트 입력 후)  
- `p`: 프롬프트 입력 (GUI)  
- `g`: 제스처 인식 모드 토글  
- `c`: 카메라 전환  
- `r`: 결과 이미지 다시 보기  
- `s`: 결과 저장  
- `q`: 종료  

---